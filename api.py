import atexit
import json
import os
import traceback
from datetime import datetime
from multiprocessing import Process

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from dotenv import load_dotenv
from flask import Flask, request, jsonify

from biz.ai.code_reviewer import CodeReviewer
from biz.ai.reporter import Reporter
from biz.gitlab.webhook_handler import MergeRequestHandler, PushHandler
from biz.utils.dingtalk import DingTalkNotifier
from biz.utils.feishu import FeishuNotifier
from biz.utils.log import logger
from biz.utils.wecom import WeComNotifier

load_dotenv()
app = Flask(__name__)


@app.route('/review/daily_report', methods=['GET'])
def daily_report():
    data_dir = os.getenv('REPORT_DATA_DIR', './')
    data_file = "push_" + datetime.now().strftime("%Y-%m-%d") + ".json"
    data_file_path = os.path.join(data_dir, data_file)
    data_entries = []
    if os.path.exists(data_file_path):
        with open(data_file_path, 'r', encoding='utf-8') as file:
            for line in file:
                # è§£ææ¯ä¸€è¡Œçš„ JSON å†…å®¹ï¼Œå¹¶æ·»åŠ åˆ° data_entries æ•°ç»„ä¸­
                try:
                    data_entries.append(json.loads(line))
                except json.JSONDecodeError:
                    # å¤„ç†å¯èƒ½çš„ JSON è§£ç é”™è¯¯
                    logger.error(f"Skipping invalid JSON entry: {line}")
    else:
        logger.error(f"Log file {data_file_path} does not exist.")
        return jsonify({'message': f"Log file {data_file_path} does not exist."}), 404

    # å¦‚æœæ²¡æœ‰data,ç›´æ¥è¿”å›
    if not data_entries:
        return jsonify({'message': 'No data to process.'}), 200

    # ä½¿ç”¨å­—å…¸å»é‡ (author, message) ç›¸åŒçš„æäº¤è®°å½•
    unique_commits = {}
    for entry in data_entries:
        author = entry.get("author", "Unknown Author")
        message = entry.get("message", "").strip()
        if (author, message) not in unique_commits:
            unique_commits[(author, message)] = {"author": author, "message": message}

    # è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼ï¼Œå¹¶æŒ‰ç…§ author æ’åº
    commits = sorted(unique_commits.values(), key=lambda x: x["author"])
    report_txt = Reporter().generate_report(json.dumps(commits))
    # å‘é’‰é’‰æ¶ˆæ¯
    send_notification(content=report_txt, msg_type="markdown", title="ä»£ç æäº¤æ—¥æŠ¥")
    return json.dumps(report_txt, ensure_ascii=False, indent=4)


# å¯åŠ¨å®šæ—¶ç”Ÿæˆæ—¥æŠ¥çš„ä»»åŠ¡
scheduler = BackgroundScheduler()
crontab_expression = os.getenv('REPORT_CRONTAB_EXPRESSION', '0 22 * * 1-5')
cron_parts = crontab_expression.split()
cron_minute, cron_hour, cron_day, cron_month, cron_day_of_week = cron_parts

# Schedule the task based on the crontab expression
scheduler.add_job(
    daily_report,
    trigger=CronTrigger(
        minute=cron_minute,
        hour=cron_hour,
        day=cron_day,
        month=cron_month,
        day_of_week=cron_day_of_week
    )
)

# Start the scheduler
scheduler.start()

# Shut down the scheduler when exiting the app
atexit.register(lambda: scheduler.shutdown())


# å¤„ç† GitLab Merge Request Webhook
@app.route('/review/webhook', methods=['POST'])
def handle_webhook():
    # è·å–è¯·æ±‚çš„JSONæ•°æ®
    if request.is_json:
        data = request.get_json()
        event_type = request.headers.get('X-Gitlab-Event')
        # ä¼˜å…ˆä»è¯·æ±‚å¤´è·å–ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è·å–
        gitlab_url = request.headers.get('X-Gitlab-Instance') or os.getenv('GITLAB_URL')
        gitlab_token = request.headers.get('X-Gitlab-Token')
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»è¯·æ±‚å¤´è·å–
        gitlab_token = os.getenv('GITLAB_ACCESS_TOKEN') or request.headers.get('X-Gitlab-Token')
        # å¦‚æœgitlab_tokenä¸ºç©ºï¼Œè¿”å›é”™è¯¯
        if not gitlab_token:
            return jsonify({'message': 'Missing GitLab access token'}), 400

        # æ‰“å°æ•´ä¸ªpayloadæ•°æ®ï¼Œæˆ–æ ¹æ®éœ€æ±‚è¿›è¡Œå¤„ç†
        logger.info(f'Received event: {event_type}')
        logger.info(f'Payload: {json.dumps(data)}')

        # å¤„ç†Merge Request Hook
        if event_type == 'Merge Request Hook':
            # åˆ›å»ºä¸€ä¸ªæ–°è¿›ç¨‹è¿›è¡Œå¼‚æ­¥å¤„ç†
            process = Process(target=handle_merge_request_event, args=(data, gitlab_token, gitlab_url))
            process.start()
            # ç«‹é©¬è¿”å›å“åº”
            return jsonify({'message': 'Request received, will process asynchronously.'}), 200
        elif event_type == 'Push Hook':
            # åˆ›å»ºä¸€ä¸ªæ–°è¿›ç¨‹è¿›è¡Œå¼‚æ­¥å¤„ç†
            process = Process(target=handle_push_event, args=(data, gitlab_token, gitlab_url))
            process.start()
            # ç«‹é©¬è¿”å›å“åº”
            return jsonify({'message': 'Request received, will process asynchronously.'}), 200
        else:
            return jsonify({'message': 'Event type not supported'}), 400
    else:
        return jsonify({'message': 'Invalid data format'}), 400


def handle_push_event(webhook_data: dict, gitlab_token: str, gitlab_url: str):
    try:
        handler = PushHandler(webhook_data, gitlab_token, gitlab_url)
        logger.info('Push Hook event received')
        commits = handler.get_push_commits()
        if not commits:
            logger.error('Failed to get commits')
            return jsonify({'message': 'Failed to get commits'}), 500
        # è®°å½•åˆ°æ•°æ®æ–‡ä»¶ä¸­
        commits_filtered = [{'message': commit['message'], 'author': commit['author'], 'timestamp': commit['timestamp']}
                            for commit in commits]
        data_dir = os.getenv('REPORT_DATA_DIR', './')
        push_data_file = "push_" + datetime.now().strftime("%Y-%m-%d") + ".json"
        push_file_path = os.path.join(data_dir, push_data_file)
        with open(push_file_path, 'a', encoding='utf-8') as f:
            for commit in commits_filtered:
                f.write(json.dumps(commit, ensure_ascii=False) + "\n")

        # æ„å»º Markdown æ ¼å¼çš„é’‰é’‰æ¶ˆæ¯
        dingtalk_msg = f"### ğŸš€ {webhook_data['project']['name']}: Push\n\n"
        dingtalk_msg += "#### æäº¤è®°å½•:\n"

        for commit in commits:
            message = commit.get('message', '').strip()
            author = commit.get('author', 'Unknown Author')
            timestamp = commit.get('timestamp', '')
            url = commit.get('url', '#')

            dingtalk_msg += (
                f"- **æäº¤ä¿¡æ¯**: {message}\n"
                f"- **æäº¤è€…**: {author}\n"
                f"- **æ—¶é—´**: {timestamp}\n"
                f"- [æŸ¥çœ‹æäº¤è¯¦æƒ…]({url})\n\n\n\n"
            )

        send_notification(content=dingtalk_msg, msg_type='markdown',
                          title=f"{webhook_data['project']['name']} Push Event")
    except Exception as e:
        error_message = f'æœåŠ¡å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}\n{traceback.format_exc()}'
        send_notification(error_message)
        logger.error('å‡ºç°æœªçŸ¥é”™è¯¯: %s', error_message)


def handle_merge_request_event(webhook_data: dict, gitlab_token: str, gitlab_url: str):
    '''
    å¤„ç†Merge Request Hookäº‹ä»¶
    :param webhook_data:
    :param gitlab_token:
    :param gitlab_url:
    :return:
    '''
    try:
        # è§£æWebhookæ•°æ®
        handler = MergeRequestHandler(webhook_data, gitlab_token, gitlab_url)
        logger.info('Merge Request Hook event received')

        if (handler.action in ['open', 'update']):  # ä»…ä»…åœ¨MRåˆ›å»ºæˆ–æ›´æ–°æ—¶è¿›è¡ŒCode Review
            # è·å–Merge Requestçš„changes
            changes = handler.get_merge_request_changes()
            logger.info('changes: %s', changes)
            if not changes:
                logger.info('æœªæ£€æµ‹åˆ°æœ‰å…³ä»£ç çš„ä¿®æ”¹,ä¿®æ”¹æ–‡ä»¶å¯èƒ½ä¸æ»¡è¶³SUPPORTED_EXTENSIONSã€‚')
                return jsonify({
                    'message': 'No code modifications were detected, the modified file may not satisfy SUPPORTED_EXTENSIONS.'}), 500
            # è·å–Merge Requestçš„commits
            commits = handler.get_merge_request_commits()
            if not commits:
                logger.error('Failed to get commits')
                return jsonify({'message': 'Failed to get commits'}), 500

            # review ä»£ç 
            commits_text = ';'.join(commit['title'] for commit in commits)
            review_result = review_code(str(filter_changes(changes)), commits_text)

            # å°†reviewç»“æœæäº¤åˆ°Gitlabçš„ notes
            handler.add_merge_request_notes(f'Auto Review Result: {review_result}')

            # æ„å»º Markdown æ ¼å¼çš„é’‰é’‰æ¶ˆæ¯
            dingtalk_msg = f"### ğŸ”€ {webhook_data['project']['name']}: Merge Request\n\n"
            dingtalk_msg += f"#### åˆå¹¶è¯·æ±‚ä¿¡æ¯:\n"

            dingtalk_msg += (
                f"- **æäº¤è€…:** {webhook_data['user']['name']}\n\n"
                f"- **æºåˆ†æ”¯**: `{webhook_data['object_attributes']['source_branch']}`\n"
                f"- **ç›®æ ‡åˆ†æ”¯**: `{webhook_data['object_attributes']['target_branch']}`\n"
                f"- **æ›´æ–°æ—¶é—´**: {webhook_data['object_attributes']['updated_at']}\n"
                f"- **æäº¤ä¿¡æ¯:** {commits_text}\n\n"
                f"- [æŸ¥çœ‹åˆå¹¶è¯¦æƒ…]({webhook_data['object_attributes']['url']})\n\n"
                f"- **AI Review ç»“æœ:** {review_result}"
            )
            send_notification(content=dingtalk_msg, msg_type='markdown', title='Merge Request Review')
        else:
            logger.info(f"Merge Request Hook event, action={handler.action}, ignored.")

    except Exception as e:
        error_message = f'AI Code Review æœåŠ¡å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}\n{traceback.format_exc()}'
        send_notification(error_message)
        logger.error('å‡ºç°æœªçŸ¥é”™è¯¯: %s', error_message)


def filter_changes(changes: list):
    '''
    è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™æ”¯æŒçš„æ–‡ä»¶ç±»å‹ä»¥åŠå¿…è¦çš„å­—æ®µä¿¡æ¯
    '''
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS = os.getenv('SUPPORTED_EXTENSIONS', '.java,.py,.php').split(',')
    # è¿‡æ»¤ `new_path` ä»¥æ”¯æŒçš„æ‰©å±•åç»“å°¾çš„å…ƒç´ , ä»…ä¿ç•™diffå’Œnew_pathå­—æ®µ
    filtered_changes = [
        {'diff': item['diff'], 'new_path': item['new_path']}
        for item in changes
        if any(item.get('new_path', '').endswith(ext) for ext in SUPPORTED_EXTENSIONS)
    ]
    return filtered_changes


# åˆ†æ–‡ä»¶reviewä»£ç 
# def review_code(data: dict):
#     changes = data.get('changes', [])
#
#     # å¦‚æœè¶…é•¿ï¼Œå–å‰REVIEW_MAX_LENGTHå­—ç¬¦
#     review_max_length = int(os.getenv('REVIEW_MAX_LENGTH', 5000))
#     review_result = []
#     # å¦‚æœchangesä¸ºç©º,æ‰“å°æ—¥å¿—
#     if not changes:
#         logger.info('ä»£ç ä¸ºç©º, data = %', str(data))
#         return 'ä»£ç ä¸ºç©º'
#
#     for change in changes:
#         new_path = change.get('new_path', '')
#         diff = change.get('diff', '')
#         parser = GitDiffParser(diff)
#
#         old_code = parser.get_old_code()
#         new_code = parser.get_new_code()
#
#         content = {
#             'æ–‡ä»¶å': new_path,
#             'ä¿®æ”¹å‰ä»£ç ': old_code,
#             'ä¿®æ”¹åä»£ç ': new_code,
#         }
#         content_str = str(content)
#
#         if len(content_str) > review_max_length:
#             content_str = content_str[:review_max_length]
#             logger.info(f'æ–‡æœ¬è¶…é•¿ï¼Œæˆªæ®µåcontent: {content_str}')
#
#         review_result.append(CodeReviewer().review_code(content_str))
#     return str(review_result)

# def review_code(data: dict):
def review_code(changes_text: str, commits_text: str = '') -> str:
    # å¦‚æœè¶…é•¿ï¼Œå–å‰REVIEW_MAX_LENGTHå­—ç¬¦
    review_max_length = int(os.getenv('REVIEW_MAX_LENGTH', 5000))
    # å¦‚æœchangesä¸ºç©º,æ‰“å°æ—¥å¿—
    if not changes_text:
        logger.info('ä»£ç ä¸ºç©º, diffs_text = %', str(changes_text))
        return 'ä»£ç ä¸ºç©º'

    if len(changes_text) > review_max_length:
        changes_text = changes_text[:review_max_length]
        logger.info(f'æ–‡æœ¬è¶…é•¿ï¼Œæˆªæ®µåcontent: {changes_text}')

    return CodeReviewer().review_code(changes_text, commits_text)


def send_notification(content, msg_type='text', title="é€šçŸ¥", is_at_all=False):
    """
    å‘é€é€šçŸ¥æ¶ˆæ¯åˆ°é…ç½®çš„å¹³å°(é’‰é’‰å’Œä¼ä¸šå¾®ä¿¡)
    :param content: æ¶ˆæ¯å†…å®¹
    :param msg_type: æ¶ˆæ¯ç±»å‹ï¼Œæ”¯æŒtextå’Œmarkdown
    :param title: æ¶ˆæ¯æ ‡é¢˜(markdownç±»å‹æ—¶ä½¿ç”¨)
    :param is_at_all: æ˜¯å¦@æ‰€æœ‰äºº
    """
    # é’‰é’‰æ¨é€
    notifier = DingTalkNotifier()
    notifier.send_message(content=content, msg_type=msg_type, title=title, is_at_all=is_at_all)

    # ä¼ä¸šå¾®ä¿¡æ¨é€
    wecom_notifier = WeComNotifier()
    wecom_notifier.send_message(content=content, msg_type=msg_type, title=title, is_at_all=is_at_all)

    # é£ä¹¦æ¨é€
    feishu_notifier = FeishuNotifier()
    feishu_notifier.send_message(content=content, msg_type=msg_type, title=title, is_at_all=is_at_all)


if __name__ == '__main__':
    port = int(os.environ.get('SERVER_PORT', 5001))
    app.run(host='0.0.0.0', port=port)
